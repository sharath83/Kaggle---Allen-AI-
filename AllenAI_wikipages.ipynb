{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from math import log\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare tfidf index for corpus (ck12 text books of relevant topics downloaded from web)\n",
    "# read data\n",
    "# get question from every row\n",
    "# get the closest paragraphs for the question\n",
    "# get the best matched answer for the closest paragraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocess the line. \n",
    "def preprocess(line):\n",
    "    #remove stop words, all symbols and numbers and split the line in to words\n",
    "    line = re.sub(\"[^a-zA-Z]\",\" \",line)\n",
    "    line = line.lower().split()\n",
    "    \n",
    "    #remove stopwords\n",
    "    stops = stopwords.words(\"english\")\n",
    "    line = [word for word in line if not word in stops]\n",
    "    \n",
    "    #stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    line = [stemmer.stem(word) for word in line]\n",
    "    \n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get closest paragraphs from the corpus given a qst and tfidf of corpus\n",
    "def get_closest_para_for_qst(qst, para_tf, idf):\n",
    "    matched_para = []\n",
    "    for para_name, para in para_tf.items():\n",
    "        w_in_para_score = 0\n",
    "        for word in qst:\n",
    "            if word in para:\n",
    "                w_in_para_score += para_tf[para_name][word] * idf[word]\n",
    "        \n",
    "        if w_in_para_score > 0:\n",
    "            matched_para.append((para, w_in_para_score))\n",
    "    \n",
    "    #Get best matched para for the qst. Paragraph with highest score are the best matched ones with the qst\n",
    "    matched_para = sorted(matched_para, key = lambda k: k[1], reverse = True)\n",
    "    return matched_para[:3] #Return top 3 matched paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the corpus - ck12 text books\n",
    "path = '/Users/homw/Documents/petp/AllenAI/wiki/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "# Initialize a dictionary to keep {line1:{word1:tf, word2:tf...}...}\n",
    "\n",
    "total_words = 0\n",
    "para_tf = {}\n",
    "num = 1\n",
    "for fname in os.listdir(path):\n",
    "    if fname.endswith(\".txt\"):\n",
    "        #print(fname)\n",
    "        file = os.path.join(path, fname)\n",
    "        for index, line in enumerate(open(file)):\n",
    "\n",
    "            line = preprocess(line)\n",
    "\n",
    "            if len(line) > 5:\n",
    "                total_line_words = 0 #To keep the count of words in paragraph/line\n",
    "                dic = {}\n",
    "                for word in line:\n",
    "                    vocab.add(word)\n",
    "                    dic.setdefault(word,0) #add a word to dictionary only if it is not existing\n",
    "                    dic[word] = dic[word]+1\n",
    "                    total_words = total_words + 1\n",
    "                    total_line_words += 1\n",
    "\n",
    "                # Compute term freq for each word in a paragraph\n",
    "                for word, count in dic.items():\n",
    "                    dic[word] = 0.5 + 0.5*(count/max(dic.values()))\n",
    "\n",
    "                # store Tf values of each paragraph in a dictionary\n",
    "                para_name = \"para\"+ str(num)\n",
    "                para_tf[para_name] = dic\n",
    "                num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute idf values for all the words in vocabulary\n",
    "idf = {}\n",
    "for word in list(vocab):\n",
    "    docs_has_word = 1\n",
    "    for index,doc in para_tf.items():\n",
    "        if word in doc:\n",
    "            docs_has_word += 1\n",
    "    idf[word] = log(len(para_tf)/docs_has_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "closest_paras = get_closest_para_for_qst(qst, para_tf, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read the data \n",
    "data = pd.read_csv(\"/Users/homw/Documents/petp/AllenAI/validation_set.tsv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qst = data.question[0]\n",
    "qst = preprocess(qst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67611"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 499\n",
      "1000 999\n",
      "1500 1499\n",
      "2000 1999\n",
      "2500 2499\n",
      "3000 2999\n",
      "3500 3499\n",
      "4000 3999\n",
      "4500 4499\n",
      "5000 4999\n",
      "5500 5499\n",
      "6000 5999\n",
      "6500 6499\n",
      "7000 6999\n",
      "7500 7499\n",
      "8000 7999\n"
     ]
    }
   ],
   "source": [
    "prediction = []\n",
    "missed = 0\n",
    "for index, record in data.iterrows():\n",
    "    #print(index, record)\n",
    "    qst = preprocess(record[\"question\"])\n",
    "    closest_paras = get_closest_para_for_qst(qst, para_tf, idf) #Get only the paragraph, score is not required\n",
    "    # Now check which of the options out of A,B,C,D scores highest with the best matched paragraphs of the Qst\n",
    "    \n",
    "    opt_A = preprocess(record[\"answerA\"])\n",
    "    score_A = 0\n",
    "    for word in opt_A:\n",
    "        for para, score in list(closest_paras):\n",
    "            if word in para:\n",
    "                score_A += para[word] * idf[word]\n",
    "                \n",
    "    \n",
    "    opt_B = preprocess(record[\"answerB\"])\n",
    "    score_B = 0\n",
    "    for word in opt_B:\n",
    "        for para, score in list(closest_paras):\n",
    "            if word in para:\n",
    "                score_B += para[word] * idf[word]\n",
    "\n",
    "                \n",
    "    \n",
    "    opt_C = preprocess(record[\"answerC\"])\n",
    "    score_C = 0\n",
    "    for word in opt_C:\n",
    "        for para, score in list(closest_paras):\n",
    "            if word in para:\n",
    "                score_C += para[word] * idf[word]\n",
    "\n",
    "                \n",
    "    \n",
    "    opt_D = preprocess(record[\"answerD\"])\n",
    "    score_D = 0\n",
    "    for word in opt_D:\n",
    "        for para, score in list(closest_paras):\n",
    "            if word in para:\n",
    "                score_D += para[word] * idf[word]\n",
    "\n",
    "    if all([score_A,score_B,score_C,score_D]) == 0:\n",
    "        prediction.append(\"N\")\n",
    "        missed += 1\n",
    "    else:\n",
    "        prediction.append([\"A\",\"B\",\"C\",\"D\"] [np.argmax([score_A,score_B,score_C,score_D])])\n",
    "    if len(prediction)%500 == 0:\n",
    "        print(len(prediction), index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "640"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([1 for i, j in zip(prediction, data.correctAnswer) if i == j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = [\"A\",\"N\",\"C\"]\n",
    "len([1 for c in prediction if c == \"N\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#prediction = \n",
    "prediction = [\"B\" if c == \"N\" else c for c in prediction]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "# Initialize a dictionary to keep {line1:{word1:tf, word2:tf...}...}\n",
    "\n",
    "total_words = 0\n",
    "para_tf = {}\n",
    "for index, line in enumerate(open(\"/Users/homw/Documents/petp/AllenAI/wiki/science_experiments.txt\")):\n",
    "\n",
    "    line = preprocess(line)\n",
    "\n",
    "    if len(line) > 5:\n",
    "        total_line_words = 0 #To keep the count of words in paragraph/line\n",
    "        dic = {}\n",
    "        for word in line:\n",
    "            vocab.add(word)\n",
    "            dic.setdefault(word,0) #add a word to dictionary only if it is not existing\n",
    "            dic[word] = dic[word]+1\n",
    "            total_words = total_words + 1\n",
    "            total_line_words += 1\n",
    "\n",
    "        # Compute term freq for each word in a paragraph\n",
    "        #for word, count in dic.items():\n",
    "            #dic[word] = count\n",
    "\n",
    "        # store Tf values of each paragraph in a dictionary\n",
    "        para_name = \"para\"+ str(index)\n",
    "        para_tf[para_name] = dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'id': list(data['id']), 'correctAnswer': prediction})[['id', 'correctAnswer']].to_csv(\"sub2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all([0,0,0])==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(prediction == data[\"correctAnswer\"])/2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "para_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "line = \"I am 2.5 inch shorter than the random guy x@# as678\"\n",
    "line = re.sub(\"[^a-zA-Z]\",\" \",line)\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9.99954597e-01,   4.53978684e-05,   5.60254205e-09])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d=0\n",
    "\n",
    "probs = [np.exp(s) for s in scores]\n",
    "probs = probs/sum(probs)\n",
    "probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores=[20,10,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.00000000e+00,  -1.90000000e+00,  -1.80000000e+00,\n",
       "         -1.70000000e+00,  -1.60000000e+00,  -1.50000000e+00,\n",
       "         -1.40000000e+00,  -1.30000000e+00,  -1.20000000e+00,\n",
       "         -1.10000000e+00,  -1.00000000e+00,  -9.00000000e-01,\n",
       "         -8.00000000e-01,  -7.00000000e-01,  -6.00000000e-01,\n",
       "         -5.00000000e-01,  -4.00000000e-01,  -3.00000000e-01,\n",
       "         -2.00000000e-01,  -1.00000000e-01,   1.77635684e-15,\n",
       "          1.00000000e-01,   2.00000000e-01,   3.00000000e-01,\n",
       "          4.00000000e-01,   5.00000000e-01,   6.00000000e-01,\n",
       "          7.00000000e-01,   8.00000000e-01,   9.00000000e-01,\n",
       "          1.00000000e+00,   1.10000000e+00,   1.20000000e+00,\n",
       "          1.30000000e+00,   1.40000000e+00,   1.50000000e+00,\n",
       "          1.60000000e+00,   1.70000000e+00,   1.80000000e+00,\n",
       "          1.90000000e+00,   2.00000000e+00,   2.10000000e+00,\n",
       "          2.20000000e+00,   2.30000000e+00,   2.40000000e+00,\n",
       "          2.50000000e+00,   2.60000000e+00,   2.70000000e+00,\n",
       "          2.80000000e+00,   2.90000000e+00,   3.00000000e+00,\n",
       "          3.10000000e+00,   3.20000000e+00,   3.30000000e+00,\n",
       "          3.40000000e+00,   3.50000000e+00,   3.60000000e+00,\n",
       "          3.70000000e+00,   3.80000000e+00,   3.90000000e+00,\n",
       "          4.00000000e+00,   4.10000000e+00,   4.20000000e+00,\n",
       "          4.30000000e+00,   4.40000000e+00,   4.50000000e+00,\n",
       "          4.60000000e+00,   4.70000000e+00,   4.80000000e+00,\n",
       "          4.90000000e+00,   5.00000000e+00,   5.10000000e+00,\n",
       "          5.20000000e+00,   5.30000000e+00,   5.40000000e+00,\n",
       "          5.50000000e+00,   5.60000000e+00,   5.70000000e+00,\n",
       "          5.80000000e+00,   5.90000000e+00],\n",
       "       [  1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00],\n",
       "       [  2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(-2.0, 6.0, 0.1)\n",
    "scores = np.vstack([x, np.ones_like(x), 0.2 * np.ones_like(x)])\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
